"""
æ•°æ®å¯¼å…¥DBç¤ºä¾‹ - å°†æœ¬åœ°æ•°æ®å¯¼å…¥SQLiteæ•°æ®åº“

æ”¯æŒçš„æ•°æ®æ ¼å¼:
  - CSV  (.csv)      - æœ€å¸¸ç”¨çš„æ–‡æœ¬æ ¼å¼
  - Excel (.xlsx/.xls) - Microsoft Excelæ ¼å¼
  - JSON (.json)     - JavaScriptå¯¹è±¡è¡¨ç¤ºæ³•
  - Parquet (.parquet) - åˆ—å¼å­˜å‚¨ï¼Œé€‚åˆå¤§æ•°æ®
  - Feather (.feather) - é«˜æ€§èƒ½äºŒè¿›åˆ¶æ ¼å¼
  - Pickle (.pkl)    - Pythonåºåˆ—åŒ–æ ¼å¼

æ•°æ®åº“ä½ç½®: data_cache/backtest_data.db
"""

import pandas as pd
import sqlite3
import os
from datetime import datetime
from typing import Optional


# ==================== æ•°æ®å­—æ®µè¯´æ˜ ====================

"""
==================== TICKæ•°æ®å­—æ®µï¼ˆCTPåŸå§‹å­—æ®µåï¼‰====================

å¿…éœ€å­—æ®µ:
    datetime        - æ—¶é—´æˆ³ï¼Œæ ¼å¼: '2025-12-11 10:30:25.500' æˆ– datetimeå¯¹è±¡
    LastPrice       - æœ€æ–°ä»·ï¼ˆæœ€è¿‘æˆäº¤ä»·ï¼‰
    BidPrice1       - ä¹°ä¸€ä»·ï¼ˆæœ€ä¼˜ä¹°ä»·ï¼‰
    AskPrice1       - å–ä¸€ä»·ï¼ˆæœ€ä¼˜å–ä»·ï¼‰
    BidVolume1      - ä¹°ä¸€é‡
    AskVolume1      - å–ä¸€é‡
    Volume          - ç´¯è®¡æˆäº¤é‡
    OpenInterest    - æŒä»“é‡

å¯é€‰å­—æ®µï¼ˆå®Œæ•´CTPè¡Œæƒ…ï¼‰:
    TradingDay      - äº¤æ˜“æ—¥ï¼Œæ ¼å¼: '20251211'
    InstrumentID    - åˆçº¦ä»£ç ï¼Œå¦‚ 'rb2601'
    ExchangeID      - äº¤æ˜“æ‰€ä»£ç ï¼Œå¦‚ 'SHFE'
    PreSettlementPrice - æ˜¨ç»“ç®—ä»·
    PreClosePrice   - æ˜¨æ”¶ç›˜ä»·
    PreOpenInterest - æ˜¨æŒä»“é‡
    OpenPrice       - ä»Šå¼€ç›˜ä»·
    HighestPrice    - æœ€é«˜ä»·
    LowestPrice     - æœ€ä½ä»·
    Turnover        - æˆäº¤é‡‘é¢
    UpdateTime      - æ›´æ–°æ—¶é—´ï¼Œæ ¼å¼: '10:30:25'
    UpdateMillisec  - æ¯«ç§’æ•°ï¼Œå¦‚ 500
    UpperLimitPrice - æ¶¨åœä»·
    LowerLimitPrice - è·Œåœä»·

æ•°æ®åº“è¡¨åæ ¼å¼: {symbol}_tick
    ä¾‹å¦‚: rb888_tick, au888_tick, IF2601_tick


==================== Kçº¿æ•°æ®å­—æ®µ ====================

å¿…éœ€å­—æ®µ:
    datetime        - Kçº¿æ—¶é—´æˆ³ï¼Œæ ¼å¼: '2025-12-11 10:30:00' æˆ– datetimeå¯¹è±¡
    open            - å¼€ç›˜ä»·
    high            - æœ€é«˜ä»·
    low             - æœ€ä½ä»·
    close           - æ”¶ç›˜ä»·
    volume          - æˆäº¤é‡

å¯é€‰å­—æ®µ:
    amount          - æˆäº¤é¢
    open_interest   - æŒä»“é‡ï¼ˆæœŸè´§ï¼‰
    symbol          - åˆçº¦ä»£ç 

æ•°æ®åº“è¡¨åæ ¼å¼: {symbol}_{period}_{adjust}
    ä¾‹å¦‚: rb888_1m_hfq (åå¤æƒ), rb888_1h_raw (ä¸å¤æƒ), rb888_D_hfq (æ—¥çº¿åå¤æƒ)
    
    periodå–å€¼: 1m, 5m, 15m, 30m, 1h, 4h, D, W, M
    adjustå–å€¼: hfq (åå¤æƒ), raw (ä¸å¤æƒ)
"""


# ==================== æ”¯æŒçš„æ–‡ä»¶æ ¼å¼ ====================
SUPPORTED_FORMATS = {
    '.csv': 'CSV (é€—å·åˆ†éš”)',
    '.xlsx': 'Excel 2007+ (.xlsx)',
    '.xls': 'Excel 97-2003 (.xls)',
    '.json': 'JSON (JavaScriptå¯¹è±¡è¡¨ç¤ºæ³•)',
    '.parquet': 'Parquet (åˆ—å¼å­˜å‚¨)',
    '.feather': 'Feather (é«˜æ€§èƒ½äºŒè¿›åˆ¶)',
    '.pkl': 'Pickle (Pythonåºåˆ—åŒ–)',
    '.pickle': 'Pickle (Pythonåºåˆ—åŒ–)',
}


# ==================== æ•°æ®åº“è·¯å¾„é…ç½® ====================
DB_PATH = "./data_cache/backtest_data.db"


def ensure_db_dir():
    """ç¡®ä¿æ•°æ®åº“ç›®å½•å­˜åœ¨"""
    db_dir = os.path.dirname(DB_PATH)
    if db_dir and not os.path.exists(db_dir):
        os.makedirs(db_dir)
        print(f"âœ… åˆ›å»ºç›®å½•: {db_dir}")


def get_file_format(file_path: str) -> Optional[str]:
    """è·å–æ–‡ä»¶æ ¼å¼"""
    _, ext = os.path.splitext(file_path.lower())
    if ext in SUPPORTED_FORMATS:
        return ext
    return None


def read_data_file(file_path: str) -> Optional[pd.DataFrame]:
    """
    è¯»å–å„ç§æ ¼å¼çš„æ•°æ®æ–‡ä»¶
    
    æ”¯æŒæ ¼å¼: CSV, Excel, JSON, Parquet, Feather, Pickle
    
    Args:
        file_path: æ–‡ä»¶è·¯å¾„
        
    Returns:
        DataFrameæˆ–Noneï¼ˆè¯»å–å¤±è´¥æ—¶ï¼‰
    """
    if not os.path.exists(file_path):
        print(f"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
        return None
    
    ext = get_file_format(file_path)
    if ext is None:
        print(f"âŒ ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼: {file_path}")
        print(f"ğŸ’¡ æ”¯æŒçš„æ ¼å¼: {', '.join(SUPPORTED_FORMATS.keys())}")
        return None
    
    try:
        print(f"ğŸ“‚ è¯»å–æ–‡ä»¶: {file_path}")
        print(f"ğŸ“‹ æ–‡ä»¶æ ¼å¼: {SUPPORTED_FORMATS[ext]}")
        
        if ext == '.csv':
            df = pd.read_csv(file_path)
            
        elif ext in ['.xlsx', '.xls']:
            # Excelæ–‡ä»¶å¯èƒ½éœ€è¦openpyxlæˆ–xlrd
            try:
                df = pd.read_excel(file_path)
            except ImportError as e:
                if 'openpyxl' in str(e):
                    print("âŒ è¯»å–Exceléœ€è¦å®‰è£…openpyxl: pip install openpyxl")
                elif 'xlrd' in str(e):
                    print("âŒ è¯»å–.xlsæ–‡ä»¶éœ€è¦å®‰è£…xlrd: pip install xlrd")
                else:
                    print(f"âŒ è¯»å–Excelå¤±è´¥: {e}")
                return None
                
        elif ext == '.json':
            # JSONå¯èƒ½æ˜¯æ•°ç»„æˆ–å¯¹è±¡æ ¼å¼
            try:
                df = pd.read_json(file_path)
            except ValueError:
                # å°è¯•æŒ‰è¡Œè¯»å–ï¼ˆJSON Linesæ ¼å¼ï¼‰
                df = pd.read_json(file_path, lines=True)
                
        elif ext == '.parquet':
            try:
                df = pd.read_parquet(file_path)
            except ImportError:
                print("âŒ è¯»å–Parquetéœ€è¦å®‰è£…pyarrow: pip install pyarrow")
                return None
                
        elif ext == '.feather':
            try:
                df = pd.read_feather(file_path)
            except ImportError:
                print("âŒ è¯»å–Featheréœ€è¦å®‰è£…pyarrow: pip install pyarrow")
                return None
                
        elif ext in ['.pkl', '.pickle']:
            df = pd.read_pickle(file_path)
            
        else:
            print(f"âŒ æœªå®ç°çš„æ ¼å¼: {ext}")
            return None
        
        print(f"âœ… æˆåŠŸè¯»å– {len(df)} æ¡è®°å½•")
        print(f"ğŸ“‹ åˆ—å: {list(df.columns)}")
        return df
        
    except Exception as e:
        print(f"âŒ è¯»å–æ–‡ä»¶å¤±è´¥: {e}")
        return None


def process_tick_dataframe(df: pd.DataFrame) -> Optional[pd.DataFrame]:
    """
    å¤„ç†TICKæ•°æ®DataFrameï¼Œç»Ÿä¸€å­—æ®µåå’Œæ ¼å¼
    
    Args:
        df: åŸå§‹DataFrame
        
    Returns:
        å¤„ç†åçš„DataFrameæˆ–None
    """
    # å­—æ®µæ˜ å°„ï¼ˆå…¼å®¹å…¶ä»–æ ¼å¼ï¼‰
    field_mapping = {
        # å°å†™ç‰ˆæœ¬
        'last_price': 'LastPrice',
        'lastprice': 'LastPrice',
        'bid_price1': 'BidPrice1',
        'bidprice1': 'BidPrice1',
        'bid1': 'BidPrice1',
        'ask_price1': 'AskPrice1',
        'askprice1': 'AskPrice1',
        'ask1': 'AskPrice1',
        'bid_volume1': 'BidVolume1',
        'bidvolume1': 'BidVolume1',
        'ask_volume1': 'AskVolume1',
        'askvolume1': 'AskVolume1',
        'open_interest': 'OpenInterest',
        'openinterest': 'OpenInterest',
        'oi': 'OpenInterest',
        # ä»·æ ¼ç›¸å…³
        'price': 'LastPrice',
        'last': 'LastPrice',
        'close': 'LastPrice',  # æœ‰äº›æ•°æ®ç”¨closeè¡¨ç¤ºæœ€æ–°ä»·
        # æˆäº¤é‡
        'vol': 'Volume',
        'qty': 'Volume',
    }
    
    # æ‰§è¡Œå­—æ®µæ˜ å°„
    df_columns_lower = {col.lower(): col for col in df.columns}
    for old_name, new_name in field_mapping.items():
        if old_name in df_columns_lower and new_name not in df.columns:
            df.rename(columns={df_columns_lower[old_name]: new_name}, inplace=True)
    
    # æ£€æŸ¥å¿…éœ€å­—æ®µ
    required_fields = ['LastPrice', 'BidPrice1', 'AskPrice1', 'Volume']
    missing_fields = [f for f in required_fields if f not in df.columns]
    
    if missing_fields:
        print(f"âŒ ç¼ºå°‘å¿…éœ€å­—æ®µ: {missing_fields}")
        print(f"ğŸ’¡ TICKæ•°æ®å¿…éœ€å­—æ®µ: datetime, LastPrice, BidPrice1, AskPrice1, Volume")
        print(f"ğŸ“‹ å½“å‰åˆ—å: {list(df.columns)}")
        return None
    
    # å¤„ç†æ—¶é—´å­—æ®µ
    if 'datetime' not in df.columns:
        # å°è¯•ä»å…¶ä»–å­—æ®µç»„åˆ
        if 'TradingDay' in df.columns and 'UpdateTime' in df.columns:
            df['datetime'] = pd.to_datetime(
                df['TradingDay'].astype(str) + ' ' + df['UpdateTime'].astype(str)
            )
            if 'UpdateMillisec' in df.columns:
                df['datetime'] = df['datetime'] + pd.to_timedelta(df['UpdateMillisec'], unit='ms')
        elif 'time' in df.columns:
            df['datetime'] = pd.to_datetime(df['time'])
        elif 'date' in df.columns:
            df['datetime'] = pd.to_datetime(df['date'])
        elif 'timestamp' in df.columns:
            df['datetime'] = pd.to_datetime(df['timestamp'])
        else:
            print("âŒ æ‰¾ä¸åˆ°æ—¶é—´å­—æ®µï¼ˆdatetime/time/date/timestamp/TradingDay+UpdateTimeï¼‰")
            return None
    else:
        df['datetime'] = pd.to_datetime(df['datetime'])
    
    # æŒ‰æ—¶é—´æ’åºï¼ˆä¸è®¾ç½®ç´¢å¼•ï¼Œä¿æŒdatetimeä¸ºæ™®é€šåˆ—ï¼Œä¸å®æ—¶è½ç›˜ä¸€è‡´ï¼‰
    df.sort_values('datetime', inplace=True)
    df.reset_index(drop=True, inplace=True)
    
    # ç»Ÿä¸€TICKå­—æ®µé¡ºåºï¼ˆä¸å®æ—¶è½ç›˜ä¿æŒä¸€è‡´ï¼‰
    # æ ‡å‡†é¡ºåº: datetime åœ¨ç¬¬ä¸€ä½ï¼Œç„¶åæ˜¯ CTP åŸå§‹å­—æ®µ
    standard_order = [
        'datetime', 'InstrumentID', 'TradingDay', 'ActionDay', 
        'UpdateTime', 'UpdateMillisec', 'LastPrice', 'Volume', 'OpenInterest',
        'BidPrice1', 'AskPrice1', 'BidVolume1', 'AskVolume1',
        'BidPrice2', 'AskPrice2', 'BidVolume2', 'AskVolume2',
        'BidPrice3', 'AskPrice3', 'BidVolume3', 'AskVolume3',
        'BidPrice4', 'AskPrice4', 'BidVolume4', 'AskVolume4',
        'BidPrice5', 'AskPrice5', 'BidVolume5', 'AskVolume5',
        'Turnover', 'PreSettlementPrice', 'PreClosePrice', 'PreOpenInterest',
        'OpenPrice', 'HighestPrice', 'LowestPrice', 'ClosePrice',
        'UpperLimitPrice', 'LowerLimitPrice', 'SettlementPrice',
        'ExchangeID', 'ExchangeInstID'
    ]
    
    # æŒ‰æ ‡å‡†é¡ºåºæ’åˆ—å·²æœ‰çš„åˆ—ï¼Œå…¶ä»–åˆ—è¿½åŠ åˆ°æœ«å°¾
    ordered_cols = []
    remaining_cols = df.columns.tolist()
    
    for col in standard_order:
        if col in remaining_cols:
            ordered_cols.append(col)
            remaining_cols.remove(col)
    
    # è¿½åŠ æ ‡å‡†é¡ºåºä¸­æ²¡æœ‰çš„å…¶ä»–åˆ—
    ordered_cols.extend(remaining_cols)
    df = df[ordered_cols]
    
    return df


def process_kline_dataframe(df: pd.DataFrame) -> Optional[pd.DataFrame]:
    """
    å¤„ç†Kçº¿æ•°æ®DataFrameï¼Œç»Ÿä¸€å­—æ®µåå’Œæ ¼å¼
    
    Args:
        df: åŸå§‹DataFrame
        
    Returns:
        å¤„ç†åçš„DataFrameæˆ–None
    """
    # å­—æ®µæ˜ å°„
    field_mapping = {
        'Open': 'open',
        'High': 'high',
        'Low': 'low',
        'Close': 'close',
        'Volume': 'volume',
        'vol': 'volume',
        'qty': 'volume',
        'amount': 'amount',
        'turnover': 'amount',
        'open_interest': 'open_interest',
        'oi': 'open_interest',
        'OpenInterest': 'open_interest',
    }
    
    # æ‰§è¡Œå­—æ®µæ˜ å°„
    for old_name, new_name in field_mapping.items():
        if old_name in df.columns and new_name not in df.columns:
            df.rename(columns={old_name: new_name}, inplace=True)
    
    # æ£€æŸ¥å¿…éœ€å­—æ®µ
    required_fields = ['open', 'high', 'low', 'close', 'volume']
    missing_fields = [f for f in required_fields if f not in df.columns]
    
    if missing_fields:
        print(f"âŒ ç¼ºå°‘å¿…éœ€å­—æ®µ: {missing_fields}")
        print(f"ğŸ’¡ Kçº¿æ•°æ®å¿…éœ€å­—æ®µ: datetime, open, high, low, close, volume")
        print(f"ğŸ“‹ å½“å‰åˆ—å: {list(df.columns)}")
        return None
    
    # å¤„ç†æ—¶é—´å­—æ®µ
    if 'datetime' not in df.columns:
        if 'date' in df.columns:
            df['datetime'] = pd.to_datetime(df['date'])
        elif 'time' in df.columns:
            df['datetime'] = pd.to_datetime(df['time'])
        elif 'timestamp' in df.columns:
            df['datetime'] = pd.to_datetime(df['timestamp'])
        else:
            print("âŒ æ‰¾ä¸åˆ°æ—¶é—´å­—æ®µï¼ˆdatetime/date/time/timestampï¼‰")
            return None
    else:
        df['datetime'] = pd.to_datetime(df['datetime'])
    
    # æŒ‰æ—¶é—´æ’åºï¼ˆä¸è®¾ç½®ç´¢å¼•ï¼Œä¿æŒdatetimeä¸ºæ™®é€šåˆ—ï¼Œä¸å®æ—¶è½ç›˜ä¸€è‡´ï¼‰
    df.sort_values('datetime', inplace=True)
    df.reset_index(drop=True, inplace=True)
    
    # ç»Ÿä¸€Kçº¿å­—æ®µé¡ºåºï¼ˆä¸APIå’Œå®æ—¶è½ç›˜ä¿æŒä¸€è‡´ï¼‰
    # æ ‡å‡†é¡ºåº: datetime, symbol, open, high, low, close, volume, amount, openint/open_interest, cumulative_openint
    standard_order = ['datetime', 'symbol', 'open', 'high', 'low', 'close', 'volume', 'amount', 'open_interest', 'openint', 'cumulative_openint']
    
    # æŒ‰æ ‡å‡†é¡ºåºæ’åˆ—å·²æœ‰çš„åˆ—ï¼Œå…¶ä»–åˆ—è¿½åŠ åˆ°æœ«å°¾
    ordered_cols = []
    remaining_cols = df.columns.tolist()
    
    for col in standard_order:
        if col in remaining_cols:
            ordered_cols.append(col)
            remaining_cols.remove(col)
    
    # è¿½åŠ æ ‡å‡†é¡ºåºä¸­æ²¡æœ‰çš„å…¶ä»–åˆ—
    ordered_cols.extend(remaining_cols)
    df = df[ordered_cols]
    
    return df


def import_tick_data(file_path: str, symbol: str, replace: bool = False) -> int:
    """
    ä»æ–‡ä»¶å¯¼å…¥TICKæ•°æ®åˆ°æ•°æ®åº“
    
    æ”¯æŒæ ¼å¼: CSV, Excel, JSON, Parquet, Feather, Pickle
    
    Args:
        file_path: æ–‡ä»¶è·¯å¾„
        symbol: å“ç§ä»£ç ï¼ˆå¦‚ 'rb888', 'au888'ï¼‰
        replace: æ˜¯å¦æ›¿æ¢å·²æœ‰æ•°æ®ï¼ˆTrue=æ›¿æ¢ï¼ŒFalse=è¿½åŠ ï¼‰
    
    Returns:
        å¯¼å…¥çš„æ•°æ®æ¡æ•°
    """
    print(f"\n{'='*60}")
    print(f"å¯¼å…¥TICKæ•°æ®")
    print(f"{'='*60}")
    
    # è¯»å–æ–‡ä»¶
    df = read_data_file(file_path)
    if df is None:
        return 0
    
    # å¤„ç†æ•°æ®
    df = process_tick_dataframe(df)
    if df is None:
        return 0
    
    print(f"ğŸ“… æ•°æ®èŒƒå›´: {df['datetime'].iloc[0]} è‡³ {df['datetime'].iloc[-1]}")
    print(f"ğŸ“Š æ•°æ®åˆ—: {list(df.columns)}")
    
    # å†™å…¥æ•°æ®åº“
    ensure_db_dir()
    table_name = f"{symbol}_tick"
    
    conn = sqlite3.connect(DB_PATH)
    try:
        if_exists = 'replace' if replace else 'append'
        # ä½¿ç”¨ index=Falseï¼Œä¸å®æ—¶è½ç›˜ä¿æŒä¸€è‡´
        df.to_sql(table_name, conn, if_exists=if_exists, index=False)
        print(f"âœ… æˆåŠŸå¯¼å…¥ {len(df)} æ¡TICKæ•°æ®åˆ°è¡¨ [{table_name}]")
        print(f"ğŸ“ æ•°æ®åº“: {DB_PATH}")
    finally:
        conn.close()
    
    return len(df)


def import_kline_data(file_path: str, symbol: str, period: str = '1m', 
                      adjust: str = 'hfq', replace: bool = False) -> int:
    """
    ä»æ–‡ä»¶å¯¼å…¥Kçº¿æ•°æ®åˆ°æ•°æ®åº“
    
    æ”¯æŒæ ¼å¼: CSV, Excel, JSON, Parquet, Feather, Pickle
    
    Args:
        file_path: æ–‡ä»¶è·¯å¾„
        symbol: å“ç§ä»£ç ï¼ˆå¦‚ 'rb888', 'au888'ï¼‰
        period: Kçº¿å‘¨æœŸï¼ˆ1m, 5m, 15m, 30m, 1h, 4h, D, W, Mï¼‰
        adjust: å¤æƒç±»å‹ï¼ˆhfq=åå¤æƒ, raw=ä¸å¤æƒï¼‰
        replace: æ˜¯å¦æ›¿æ¢å·²æœ‰æ•°æ®
    
    Returns:
        å¯¼å…¥çš„æ•°æ®æ¡æ•°
    """
    print(f"\n{'='*60}")
    print(f"å¯¼å…¥Kçº¿æ•°æ®")
    print(f"{'='*60}")
    
    # è¯»å–æ–‡ä»¶
    df = read_data_file(file_path)
    if df is None:
        return 0
    
    # å¤„ç†æ•°æ®
    df = process_kline_dataframe(df)
    if df is None:
        return 0
    
    print(f"ğŸ“… æ•°æ®èŒƒå›´: {df['datetime'].iloc[0]} è‡³ {df['datetime'].iloc[-1]}")
    
    # å†™å…¥æ•°æ®åº“
    ensure_db_dir()
    table_name = f"{symbol}_{period}_{adjust}"
    
    conn = sqlite3.connect(DB_PATH)
    try:
        if_exists = 'replace' if replace else 'append'
        # ä½¿ç”¨ index=Falseï¼Œä¸å®æ—¶è½ç›˜ä¿æŒä¸€è‡´
        df.to_sql(table_name, conn, if_exists=if_exists, index=False)
        print(f"âœ… æˆåŠŸå¯¼å…¥ {len(df)} æ¡Kçº¿æ•°æ®åˆ°è¡¨ [{table_name}]")
        print(f"ğŸ“ æ•°æ®åº“: {DB_PATH}")
    finally:
        conn.close()
    
    return len(df)


def batch_import(folder_path: str, data_type: str = 'tick', 
                 period: str = '1m', adjust: str = 'hfq', replace: bool = False):
    """
    æ‰¹é‡å¯¼å…¥æ–‡ä»¶å¤¹ä¸­çš„æ‰€æœ‰æ•°æ®æ–‡ä»¶
    
    æ–‡ä»¶åæ ¼å¼è¦æ±‚: {symbol}_xxx.csv æˆ– {symbol}.csv
    ä¾‹å¦‚: rb888_20251211.csv, au888.parquet
    
    Args:
        folder_path: æ–‡ä»¶å¤¹è·¯å¾„
        data_type: æ•°æ®ç±»å‹ ('tick' æˆ– 'kline')
        period: Kçº¿å‘¨æœŸï¼ˆä»…data_type='kline'æ—¶æœ‰æ•ˆï¼‰
        adjust: å¤æƒç±»å‹ï¼ˆä»…data_type='kline'æ—¶æœ‰æ•ˆï¼‰
        replace: æ˜¯å¦æ›¿æ¢å·²æœ‰æ•°æ®
    """
    print(f"\n{'='*60}")
    print(f"æ‰¹é‡å¯¼å…¥ - {folder_path}")
    print(f"{'='*60}")
    
    if not os.path.exists(folder_path):
        print(f"âŒ æ–‡ä»¶å¤¹ä¸å­˜åœ¨: {folder_path}")
        return
    
    # è·å–æ‰€æœ‰æ”¯æŒçš„æ–‡ä»¶
    files = []
    for ext in SUPPORTED_FORMATS.keys():
        files.extend([f for f in os.listdir(folder_path) if f.lower().endswith(ext)])
    
    if not files:
        print(f"ğŸ“­ æœªæ‰¾åˆ°æ”¯æŒçš„æ•°æ®æ–‡ä»¶")
        print(f"ğŸ’¡ æ”¯æŒçš„æ ¼å¼: {', '.join(SUPPORTED_FORMATS.keys())}")
        return
    
    print(f"ğŸ“‚ æ‰¾åˆ° {len(files)} ä¸ªæ–‡ä»¶")
    
    success_count = 0
    fail_count = 0
    
    for file_name in files:
        file_path = os.path.join(folder_path, file_name)
        
        # ä»æ–‡ä»¶åæå–symbol
        base_name = os.path.splitext(file_name)[0]
        symbol = base_name.split('_')[0]  # å–ç¬¬ä¸€éƒ¨åˆ†ä½œä¸ºsymbol
        
        print(f"\n--- å¤„ç†: {file_name} (symbol={symbol}) ---")
        
        try:
            if data_type == 'tick':
                count = import_tick_data(file_path, symbol, replace)
            else:
                count = import_kline_data(file_path, symbol, period, adjust, replace)
            
            if count > 0:
                success_count += 1
            else:
                fail_count += 1
        except Exception as e:
            print(f"âŒ å¯¼å…¥å¤±è´¥: {e}")
            fail_count += 1
    
    print(f"\n{'='*60}")
    print(f"æ‰¹é‡å¯¼å…¥å®Œæˆ: æˆåŠŸ {success_count} ä¸ª, å¤±è´¥ {fail_count} ä¸ª")
    print(f"{'='*60}")


def list_db_tables():
    """åˆ—å‡ºæ•°æ®åº“ä¸­çš„æ‰€æœ‰è¡¨"""
    if not os.path.exists(DB_PATH):
        print(f"âŒ æ•°æ®åº“ä¸å­˜åœ¨: {DB_PATH}")
        return
    
    print(f"\n{'='*60}")
    print(f"æ•°æ®åº“è¡¨åˆ—è¡¨: {DB_PATH}")
    print(f"{'='*60}")
    
    conn = sqlite3.connect(DB_PATH)
    try:
        cursor = conn.execute("SELECT name FROM sqlite_master WHERE type='table' ORDER BY name")
        tables = cursor.fetchall()
        
        if not tables:
            print("ğŸ“­ æ•°æ®åº“ä¸ºç©ºï¼Œæ²¡æœ‰è¡¨")
            return
        
        print(f"\nå…± {len(tables)} ä¸ªè¡¨:\n")
        
        for (table_name,) in tables:
            # è·å–è¡¨çš„è¡Œæ•°å’Œåˆ—ä¿¡æ¯
            count = conn.execute(f"SELECT COUNT(*) FROM [{table_name}]").fetchone()[0]
            cursor = conn.execute(f"PRAGMA table_info([{table_name}])")
            columns = [row[1] for row in cursor.fetchall()]
            
            # åˆ¤æ–­æ•°æ®ç±»å‹
            if '_tick' in table_name:
                data_type = "TICK"
            elif any(p in table_name for p in ['_1m_', '_5m_', '_15m_', '_30m_', '_1h_', '_4h_', '_D_', '_W_', '_M_']):
                data_type = "Kçº¿"
            else:
                data_type = "æœªçŸ¥"
            
            print(f"  ğŸ“Š {table_name}")
            print(f"     ç±»å‹: {data_type} | è®°å½•æ•°: {count:,}")
            print(f"     åˆ—: {', '.join(columns[:8])}{'...' if len(columns) > 8 else ''}")
            print()
    finally:
        conn.close()


def query_table_sample(table_name: str, limit: int = 5):
    """æŸ¥è¯¢è¡¨çš„ç¤ºä¾‹æ•°æ®"""
    if not os.path.exists(DB_PATH):
        print(f"âŒ æ•°æ®åº“ä¸å­˜åœ¨: {DB_PATH}")
        return
    
    print(f"\n{'='*60}")
    print(f"è¡¨ [{table_name}] ç¤ºä¾‹æ•°æ®")
    print(f"{'='*60}")
    
    conn = sqlite3.connect(DB_PATH)
    try:
        df = pd.read_sql(f"SELECT * FROM [{table_name}] LIMIT {limit}", conn)
        print(f"\nå‰ {limit} æ¡è®°å½•:")
        print(df.to_string())
    except Exception as e:
        print(f"âŒ æŸ¥è¯¢å¤±è´¥: {e}")
    finally:
        conn.close()


def create_sample_data():
    """åˆ›å»ºå„ç§æ ¼å¼çš„ç¤ºä¾‹æ•°æ®"""
    import numpy as np
    
    print("\nğŸ“ åˆ›å»ºç¤ºä¾‹æ•°æ®...")
    
    # ç”Ÿæˆç¤ºä¾‹TICKæ•°æ®
    base_time = datetime(2025, 12, 11, 9, 0, 0)
    times = [base_time + pd.Timedelta(milliseconds=500*i) for i in range(100)]
    
    base_price = 3500.0
    prices = base_price + np.cumsum(np.random.randn(100) * 2)
    
    tick_df = pd.DataFrame({
        'datetime': times,
        'LastPrice': prices,
        'BidPrice1': prices - 1,
        'AskPrice1': prices + 1,
        'BidVolume1': np.random.randint(10, 100, 100),
        'AskVolume1': np.random.randint(10, 100, 100),
        'Volume': np.cumsum(np.random.randint(1, 10, 100)),
        'OpenInterest': 100000 + np.cumsum(np.random.randint(-50, 50, 100)),
    })
    
    # ç”Ÿæˆç¤ºä¾‹Kçº¿æ•°æ®
    kline_times = [base_time + pd.Timedelta(minutes=i) for i in range(100)]
    opens = [base_price]
    for i in range(1, 100):
        opens.append(opens[-1] + np.random.randn() * 5)
    
    kline_df = pd.DataFrame({
        'datetime': kline_times,
        'open': opens,
        'high': [o + abs(np.random.randn() * 3) for o in opens],
        'low': [o - abs(np.random.randn() * 3) for o in opens],
        'close': [o + np.random.randn() * 2 for o in opens],
        'volume': np.random.randint(100, 1000, 100),
        'open_interest': 100000 + np.cumsum(np.random.randint(-100, 100, 100)),
    })
    
    # ä¿å­˜ä¸ºä¸åŒæ ¼å¼
    sample_dir = "./sample_data"
    os.makedirs(sample_dir, exist_ok=True)
    
    formats_created = []
    
    # CSV
    tick_df.to_csv(f"{sample_dir}/sample_tick.csv", index=False)
    kline_df.to_csv(f"{sample_dir}/sample_kline.csv", index=False)
    formats_created.append("CSV")
    
    # JSON
    tick_df.to_json(f"{sample_dir}/sample_tick.json", orient='records', date_format='iso')
    kline_df.to_json(f"{sample_dir}/sample_kline.json", orient='records', date_format='iso')
    formats_created.append("JSON")
    
    # Pickle
    tick_df.to_pickle(f"{sample_dir}/sample_tick.pkl")
    kline_df.to_pickle(f"{sample_dir}/sample_kline.pkl")
    formats_created.append("Pickle")
    
    # Excel (éœ€è¦openpyxl)
    try:
        tick_df.to_excel(f"{sample_dir}/sample_tick.xlsx", index=False)
        kline_df.to_excel(f"{sample_dir}/sample_kline.xlsx", index=False)
        formats_created.append("Excel")
    except ImportError:
        print("âš ï¸ Excelæ ¼å¼éœ€è¦å®‰è£…openpyxl: pip install openpyxl")
    
    # Parquet (éœ€è¦pyarrow)
    try:
        tick_df.to_parquet(f"{sample_dir}/sample_tick.parquet", index=False)
        kline_df.to_parquet(f"{sample_dir}/sample_kline.parquet", index=False)
        formats_created.append("Parquet")
    except ImportError:
        print("âš ï¸ Parquetæ ¼å¼éœ€è¦å®‰è£…pyarrow: pip install pyarrow")
    
    # Feather (éœ€è¦pyarrow)
    try:
        tick_df.to_feather(f"{sample_dir}/sample_tick.feather")
        kline_df.to_feather(f"{sample_dir}/sample_kline.feather")
        formats_created.append("Feather")
    except ImportError:
        print("âš ï¸ Featheræ ¼å¼éœ€è¦å®‰è£…pyarrow: pip install pyarrow")
    
    print(f"âœ… ç¤ºä¾‹æ•°æ®å·²ä¿å­˜åˆ°: {sample_dir}/")
    print(f"ğŸ“‹ å·²åˆ›å»ºæ ¼å¼: {', '.join(formats_created)}")
    print(f"   - sample_tick.* (TICKæ•°æ®)")
    print(f"   - sample_kline.* (Kçº¿æ•°æ®)")
    
    return sample_dir


# ==================== ä¸»ç¨‹åº ====================

if __name__ == "__main__":
    print("\n" + "="*60)
    print("æ•°æ®å¯¼å…¥DBå·¥å…·")
    print("="*60)
    print(f"\næ”¯æŒçš„æ–‡ä»¶æ ¼å¼:")
    for ext, desc in SUPPORTED_FORMATS.items():
        print(f"  {ext:10} - {desc}")
    
    # é€‰æ‹©æ“ä½œ
    print("""
è¯·é€‰æ‹©æ“ä½œ:
  1. å¯¼å…¥TICKæ•°æ®ï¼ˆå•ä¸ªæ–‡ä»¶ï¼‰
  2. å¯¼å…¥Kçº¿æ•°æ®ï¼ˆå•ä¸ªæ–‡ä»¶ï¼‰
  3. æ‰¹é‡å¯¼å…¥TICKæ•°æ®ï¼ˆæ–‡ä»¶å¤¹ï¼‰
  4. æ‰¹é‡å¯¼å…¥Kçº¿æ•°æ®ï¼ˆæ–‡ä»¶å¤¹ï¼‰
  5. åˆ›å»ºç¤ºä¾‹æ•°æ®ï¼ˆå„ç§æ ¼å¼ï¼‰
  6. æŸ¥çœ‹æ•°æ®åº“ä¸­çš„æ‰€æœ‰è¡¨
  7. æŸ¥è¯¢è¡¨çš„ç¤ºä¾‹æ•°æ®
  0. é€€å‡º
""")
    
    choice = input("è¯·è¾“å…¥é€‰é¡¹ (0-7): ").strip()
    
    if choice == '1':
        file_path = input("è¯·è¾“å…¥TICKæ•°æ®æ–‡ä»¶è·¯å¾„: ").strip()
        symbol = input("è¯·è¾“å…¥å“ç§ä»£ç  (å¦‚ rb888): ").strip()
        replace = input("æ˜¯å¦æ›¿æ¢å·²æœ‰æ•°æ®? (y/n): ").strip().lower() == 'y'
        import_tick_data(file_path, symbol, replace)
    
    elif choice == '2':
        file_path = input("è¯·è¾“å…¥Kçº¿æ•°æ®æ–‡ä»¶è·¯å¾„: ").strip()
        symbol = input("è¯·è¾“å…¥å“ç§ä»£ç  (å¦‚ rb888): ").strip()
        period = input("è¯·è¾“å…¥Kçº¿å‘¨æœŸ (1m/5m/15m/30m/1h/4h/D/W/M) [é»˜è®¤1m]: ").strip() or '1m'
        adjust = input("è¯·è¾“å…¥å¤æƒç±»å‹ (hfq/raw) [é»˜è®¤hfq]: ").strip() or 'hfq'
        replace = input("æ˜¯å¦æ›¿æ¢å·²æœ‰æ•°æ®? (y/n): ").strip().lower() == 'y'
        import_kline_data(file_path, symbol, period, adjust, replace)
    
    elif choice == '3':
        folder_path = input("è¯·è¾“å…¥TICKæ•°æ®æ–‡ä»¶å¤¹è·¯å¾„: ").strip()
        replace = input("æ˜¯å¦æ›¿æ¢å·²æœ‰æ•°æ®? (y/n): ").strip().lower() == 'y'
        batch_import(folder_path, 'tick', replace=replace)
    
    elif choice == '4':
        folder_path = input("è¯·è¾“å…¥Kçº¿æ•°æ®æ–‡ä»¶å¤¹è·¯å¾„: ").strip()
        period = input("è¯·è¾“å…¥Kçº¿å‘¨æœŸ (1m/5m/15m/30m/1h/4h/D/W/M) [é»˜è®¤1m]: ").strip() or '1m'
        adjust = input("è¯·è¾“å…¥å¤æƒç±»å‹ (hfq/raw) [é»˜è®¤hfq]: ").strip() or 'hfq'
        replace = input("æ˜¯å¦æ›¿æ¢å·²æœ‰æ•°æ®? (y/n): ").strip().lower() == 'y'
        batch_import(folder_path, 'kline', period, adjust, replace)
    
    elif choice == '5':
        sample_dir = create_sample_data()
        print(f"\nğŸ’¡ ä½ å¯ä»¥å°è¯•å¯¼å…¥è¿™äº›ç¤ºä¾‹æ–‡ä»¶æ¥æµ‹è¯•å„ç§æ ¼å¼")
    
    elif choice == '6':
        list_db_tables()
    
    elif choice == '7':
        list_db_tables()
        table_name = input("\nè¯·è¾“å…¥è¦æŸ¥è¯¢çš„è¡¨å: ").strip()
        if table_name:
            query_table_sample(table_name)
    
    elif choice == '0':
        print("ğŸ‘‹ å†è§!")
    
    else:
        print("âŒ æ— æ•ˆé€‰é¡¹")
    
    print("\n" + "="*60)
    print("æ“ä½œå®Œæˆ")
    print("="*60)
